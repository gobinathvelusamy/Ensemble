{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa97a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.feature_extraction.text import CountVectorizer  #DT does not take strings as input for the model fit step....\n",
    "from IPython.display import Image  \n",
    "#import pydotplus as pydot\n",
    "from sklearn import tree\n",
    "from os import system\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca11a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(Filename):\n",
    "    df=pd.read_csv(Filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261407e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_remove(df):\n",
    "    df1=df.dropna(inplace=False)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0642c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_datacoversion(df):\n",
    "    df1 = df.copy()\n",
    "    df1['Churn'] =df['Churn'].replace({\"No\":0, \"Yes\":1})\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d66f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_cardinality(df):\n",
    "    low_card_cols_1=df.columns[df.nunique()==1].tolist()\n",
    "    cleansed_df =df.drop(columns=low_card_cols_1)\n",
    "    return cleansed_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e39d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datatype_con(df,col_list):\n",
    "    for  i in col_list:\n",
    "        df[i]=pd.to_numeric(df[i],errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4500d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    col_list = df.select_dtypes(include='object').columns.to_list()\n",
    "    df_one_hot = pd.get_dummies(df, columns = col_list)\n",
    "    return df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17848610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_cleanse,Predict_col,split_size):\n",
    "    y=df_cleanse[Predict_col] #target\n",
    "    X=df_cleanse.drop(columns=Predict_col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size, random_state=10)\n",
    "    return (X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8670fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train,y_train,Modlename):\n",
    "    if(Modlename=='XGB'):\n",
    "        xgb_model = XGBClassifier()\n",
    "        fitted_model=xgb_model.fit(X_train, y_train)\n",
    "    elif(Modlename=='Logit'):\n",
    "        logit = LogisticRegression()\n",
    "        fitted_model=logit.fit(X_train, y_train)\n",
    "    elif(Modlename=='KNN'):\n",
    "        KNN = KNeighborsClassifier(n_neighbors= 5 , metric = 'euclidean' ) \n",
    "        fitted_model=KNN.fit(X_train, y_train)\n",
    "    elif(Modlename=='SVM'):\n",
    "        clf = svm.SVC(gamma=0.025, C=3)     \n",
    "        fitted_model=clf.fit(X_train, y_train)\n",
    "    elif(Modlename=='DTREE'):\n",
    "        dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)  \n",
    "        fitted_model=dTree.fit(X_train, y_train)  \n",
    "    elif(Modlename=='RFCL'):\n",
    "        rfcl = RandomForestClassifier(n_estimators = 50, random_state=1,max_features=12) \n",
    "        fitted_model=rfcl.fit(X_train, y_train) \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7eda051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(Fit_model,X_test,y_test):         \n",
    "    y_pred = Fit_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    accuracy_score=metrics.accuracy_score(y_test,y_pred)*100\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f61a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_procssing(df,Val_Target_var,col_list_datatype,ID_Column):\n",
    "    #Drop the Feature which is quique for each Row \n",
    "    df.drop(ID_Column,axis=1,inplace=True)\n",
    "    #Remove the Feature which is having Low Cardinality (1)\n",
    "    df1=remove_low_cardinality(df)\n",
    "    print(\"df1 Columns\",df1.columns)\n",
    "    #Convert Datatype Conversion \n",
    "    df2=datatype_con(df1,col_list_datatype)\n",
    "    print(\"df2 Columns\",df2.columns)\n",
    "    #Remove the Null Components in the feature\n",
    "    df3=null_remove(df2)\n",
    "    print(\"df3 Columns\",df3.columns)\n",
    "    #Target Variable conversion to Decimal Values\n",
    "    df4=target_datacoversion(df3)\n",
    "    #One Hot conversion for Categorial Features\n",
    "    df_final=one_hot_encode(df4)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dd5bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_performing_model(accuracy_score):\n",
    "    max_v =-99\n",
    "    best_model=''\n",
    "    for i in accuracy_score:\n",
    "        if accuracy_score[i] > max_v:\n",
    "            max_v =accuracy_score[i]\n",
    "            best_model =i\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c479a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Accept Data file form the user and analyse the features which needs to be processed\n",
    "    val_fileName = input(\"Enter Value of CSV File Name to be modelled: \")\n",
    "    df=read_file(val_fileName)\n",
    "    print(\"Data Shape\",df.shape)\n",
    "    print(\"Data Info\",df.info())\n",
    "    \n",
    "    \n",
    "    print(\"Data Cleansing/Preparation Started\")\n",
    "    print(\"Analysis data and determine features which needs to be processed(Data type Conversioin,Unique Value)\")\n",
    "    Val_Target_var=input(\"Enter Target Column for Classification: \")\n",
    "    #TelcomCustomer-Churn_2.csv\n",
    "    col_datatype_conversion =input(\"Enter List of Columns to be Converted to Decimal\")\n",
    "    ID_Column =input(\"Enter the Column which is ID and which unique for each Row\")\n",
    "    col_list_datatype_col = col_datatype_conversion.split(',')  \n",
    "    #Calling Data cleansing Function \n",
    "    df_cleansed=data_pre_procssing(df,Val_Target_var,col_list_datatype_col,ID_Column)\n",
    "    print(\"Data Cleansing/Preparation Completed\")\n",
    "    \n",
    "    #Split the Data set as training/Test data sets \n",
    "    print(\"Split data for Modelling started\")\n",
    "    val_split_size =float(input(\"Enter the Split size in Decimal\"))\n",
    "    X_train, X_test, y_train, y_test= split_data(df_cleansed,Val_Target_var,val_split_size)\n",
    "    print(\"Split data for Modelling Completed\")\n",
    "    \n",
    "    #List of Models to be trained & Scored \n",
    "    Model_list =['XGB','KNN','Logit','SVM','DTREE','RFCL']\n",
    "    model_arr={}\n",
    "    model_a_score={}\n",
    "    for i in Model_list:\n",
    "        model_arr[i]=model_fit(X_train,y_train,i)\n",
    "        model_a_score[i]=model_score(model_arr[i],X_test,y_test)\n",
    "    print(model_arr)\n",
    "    print(model_a_score)\n",
    "    best_model=best_performing_model(model_a_score)\n",
    "    print(\"********************************************\")\n",
    "    print(\"Best Model Choosed based on Accuracy Score\",best_model)\n",
    "    print(\"********************************************\")\n",
    "    a=model_arr[best_model]\n",
    "    # Storing the best Perfomance Model as pickle FIle \n",
    "    pickle.dump(a,open('models/Best_performing_model_'+best_model+'.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7424e366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Value of CSV File Name to be modelled: TelcomCustomer-Churn_2.csv\n",
      "Data Shape (7043, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   OnlineBackup      7043 non-null   object \n",
      " 2   DeviceProtection  7043 non-null   object \n",
      " 3   TechSupport       7043 non-null   object \n",
      " 4   StreamingTV       7043 non-null   object \n",
      " 5   StreamingMovies   7043 non-null   object \n",
      " 6   Contract          7043 non-null   object \n",
      " 7   PaperlessBilling  7043 non-null   object \n",
      " 8   PaymentMethod     7043 non-null   object \n",
      " 9   MonthlyCharges    7043 non-null   float64\n",
      " 10  TotalCharges      7043 non-null   object \n",
      " 11  Churn             7043 non-null   object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 660.4+ KB\n",
      "Data Info None\n",
      "Data Cleansing/Preparation Started\n",
      "Analysis data and determine features which needs to be processed(Data type Conversioin,Unique Value)\n",
      "Enter Target Column for Classification: Churn\n",
      "Enter List of Columns to be Converted to DecimalTotalCharges\n",
      "Enter the Column which is ID and which unique for each RowcustomerID\n",
      "df1 Columns Index(['OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
      "       'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',\n",
      "       'MonthlyCharges', 'TotalCharges', 'Churn'],\n",
      "      dtype='object')\n",
      "df2 Columns Index(['OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
      "       'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',\n",
      "       'MonthlyCharges', 'TotalCharges', 'Churn'],\n",
      "      dtype='object')\n",
      "df3 Columns Index(['OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
      "       'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',\n",
      "       'MonthlyCharges', 'TotalCharges', 'Churn'],\n",
      "      dtype='object')\n",
      "Data Cleansing/Preparation Completed\n",
      "Split data for Modelling started\n",
      "Enter the Split size in Decimal0.20\n",
      "Split data for Modelling Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:43:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87      1046\n",
      "           1       0.62      0.56      0.59       361\n",
      "\n",
      "    accuracy                           0.80      1407\n",
      "   macro avg       0.74      0.72      0.73      1407\n",
      "weighted avg       0.79      0.80      0.80      1407\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84      1046\n",
      "           1       0.54      0.47      0.50       361\n",
      "\n",
      "    accuracy                           0.76      1407\n",
      "   macro avg       0.68      0.66      0.67      1407\n",
      "weighted avg       0.75      0.76      0.75      1407\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87      1046\n",
      "           1       0.63      0.54      0.58       361\n",
      "\n",
      "    accuracy                           0.80      1407\n",
      "   macro avg       0.74      0.71      0.72      1407\n",
      "weighted avg       0.79      0.80      0.79      1407\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1046\n",
      "           1       0.50      0.44      0.47       361\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.66      0.64      0.65      1407\n",
      "weighted avg       0.73      0.74      0.74      1407\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83      1046\n",
      "           1       0.51      0.53      0.52       361\n",
      "\n",
      "    accuracy                           0.75      1407\n",
      "   macro avg       0.67      0.68      0.67      1407\n",
      "weighted avg       0.75      0.75      0.75      1407\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87      1046\n",
      "           1       0.61      0.54      0.58       361\n",
      "\n",
      "    accuracy                           0.80      1407\n",
      "   macro avg       0.73      0.71      0.72      1407\n",
      "weighted avg       0.79      0.80      0.79      1407\n",
      "\n",
      "{'XGB': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 'KNN': KNeighborsClassifier(metric='euclidean'), 'Logit': LogisticRegression(), 'SVM': SVC(C=3, gamma=0.025), 'DTREE': DecisionTreeClassifier(random_state=1), 'RFCL': RandomForestClassifier(max_features=12, n_estimators=50, random_state=1)}\n",
      "{'XGB': 79.8862828713575, 'KNN': 75.90618336886993, 'Logit': 79.95735607675905, 'SVM': 74.4136460554371, 'DTREE': 74.84008528784648, 'RFCL': 79.53091684434968}\n",
      "********************************************\n",
      "Best Model Choosed based on Accuracy Score Logit\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0402554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
